---
description: The continuously evolving landscape of AI solutions promises to fundamentally change the way that businesses, governments, and society as a whole interact with and impact the world around us. While some of the promises of change may be years or even decades away, the technological possibilities represent exciting opportunities. As AI continues to evolve, AI practitioners must carefully consider the impact of AI on the people who interact with it and are affected by it. 
slug: ai-standards-principles-guidelines
title: "AI standards, principles and guidelines"
---

## AI standards, principles and guidelines
To really understand all of the nuances of how to embed responsibility into AI systems, there is a lot of theory to as how it relates to ethics, bias and fairness, transparency and explainability, privacy and more. The challenge with all of these topics is that the space is learning by trial, thus there are no perfect answers or approaches yet. 
{: .intro }

Some agencies have already begun to create AI policies and even principles around the ethical and responsible use of AI. These are important to have, but the next step is to translate these principles into actionable steps that agencies can implement throughout the AI development process.

## What does it mean to build responsible AI in practice?
Building responsible AI systems requires embedding critical thinking and ethics into every decision made throughout the process. This means in the design process, but also in the development phase and continued revisiting of the model once it has been deployed to production. Certainly, human centered design is at the core of responsible AI design, but even decisions such as how to handle missing values in the training data, how to split data into training, test and validation sets and which metrics will be used to measure the performance of a model have implications for the outcome that the AI system will produce and thus are important to think through. 

These may seem like deeply technical questions that a leader, program or project manager does not normally focus on--and that is exactly the point. Successful management of an AI project means that ethical and responsible AI decisions are not just made by AI ethicists--they are made each and every day by designers, project and program managers, and developers working together. These stakeholders must all understand that many of the day-to-day decisions they make are indeed ethical decisions.

Embedding responsible AI in practice means that all roles--technical and non-technical, feds and vendors-- in the AI development lifecycle are trained on responsibility and empowered and expected to be accountable for the decisions they make. The question becomes, “How do we ensure that all of these roles have the tools they need to make good decisions--or at a very minimum document these decisions?”. This is no easy task. 

## Embed responsibility in every decision
As all roles have the duty of ensuring that we are building and deploying responsible AI, we can start by thinking practically about what we are building. Starting with the team itself, building diverse, interdisciplinary teams are an elemental step building responsible AI systems. Some of these are a good idea to think about for ALL projects, not just AI projects--but are especially important for AI projects to avoid some of the pitfalls that have led to problematic AI systems.

### Responsible AI needs to:

#### 1. Be focused
Research projects and pilots of new technologies can be exploratory, often loosely defining the expected outcome with the hopes of gaining insight into the given topic or process being studied. While this can certainly produce meaningful output, teams that are building models and and of AI systems need to clearly understand the problem to be solved, who is affected by this problem and how AI may--or may not--be a solution. Why are you even bothering to build an AI solution in the first place? Is it the only option for solving this particular problem? Will it actually solve the problem? Will it benefit all users or just some? Ensuring there is a clear and coherent focus means creating a team environment in which all stakeholders are educated and able to participate and evaluate. 

### 2. Be accountable
AI systems cannot exist in isolation. The outcomes produced by these systems must be justifiable to the users who interact with it. This may also mean, depending on the consequences of or the type of decision being made, the system must be able to explain how the answer is reached. Do I necessarily need to know why an AI system recommended that I buy a blue shirt vs. a red one? Probably not. Do I need to know how an AI system reached the conclusion that one person should receive a home loan vs. another? Absolutely. This also means that the decisions that go into the creation of the system need to be owned by a person or team. When something deviates from the intended output or behavior, who is responsible for noticing and correcting this? Is someone responsible for making sure that every step is done correctly, not just done?

This starts with establishing clear roles and responsibility for data and model management so that, at a minimum, an aberrant outcome can be linked to its training source. This is often significantly harder than you would think, especially in the case of deep learning.

### 3. Avoid harm, including inaccuracy &amp; bias
Unintended bias can enter a model in many ways. Though it may be impossible to completely eliminate bias--and that may not even be the goal--an AI system should have a mechanism to evaluate for bias in datasets and in the outcomes the system produces to ensure that the output is not disproportionately affecting users based on race, gender, sexual orientation, religion, or otherwise.

### 4. Be monitored
Ultimately, AI systems should undergo rigorous monitoring and evaluation. This exists in the form of structured oversight mechanisms and policies to be able to identify anything that is potentially causing a problem so that intervention can happen quickly. Without oversight, how would we ever know if something went wrong? Are we creating a situation where we are trapped by our own blindspots? Certainly, oversight does not solve all potential problems that may arise with an AI system, but it does create a plan to iteratively watch for and hopefully catch problems before they become harmful.

## Ask questions often and repeatedly
As this is not yet a solved problem, the best way to start down the path of responsible implementation of AI is to ask questions. Ask them early, Ask them often. Ask the same question over and over again. Sometimes, you may not have an answer to a question--that’s ok. Make a plan for getting the answer as the project progresses. 

![](../images/ai-design-develop-deploy-loop.png)

As each project is slightly different, the questions required to assess for responsibility may be different. The questions outlined in this module are designed to guide teams that are building and implementing AI systems. They are not standards. They are not policy. They are good questions to consider as the team progresses through the stages of the AI lifecycle to begin to embed responsible AI in the process. The questions are designed to foster discussions around broad ethics topics. These questions should be iteratively revisited through the design-develop-deploy cycle and extensively tested to help mitigate unintended consequences from the application of AI.

### A note on maturity: when to start asking questions
Many think that responsible implementation of AI is only for complex systems ready for production. That is too late to start thinking about these questions. Even if you are one person playing around with data to see if there is a possibility that AI might be a possible solution--start asking these questions. Some of the questions may not apply in the early stages of discovery. That’s ok. Keep them on the road map so that you can eventually answer them as the project grows. Keeping documented answers to these questions will also help you track the progress of your project and will help identify when in the AI lifecycle these questions become most relevant.