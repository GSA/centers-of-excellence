---
description: Automating text transcription from standardized health insurance documents
slug: automating-text-transcriptions
title: "Automating text transcription from standardized health insurance documents"
---
The U.S. Bureau of Labor Statistics (BLS) is exploring how to automate the extraction and classification of items from Summary of Benefits and Coverage (SBC) PDF documents relevant to data collection in the National Compensation Survey (NCS). 
{: .intro }

This automated text extraction system would be used alongside current data entry tools with the expectation that it would be fully integrated into the data entry system over time.

The team is working to identify key pieces of data that need to be extracted from SBCs and then developing unique (though in often cases similar) solutions for the labeling and placement of these items into a tabular dataset. For three data elements (overall deductible, out-of-pocket maximum payments, and individual responsibilities for specific procedures), the team uses a Named Entity Recognition approach to label dollar or percent values as belonging to specific, NCS-relevant categories. Each of these data elements has its own trained machine learning model. They are still evaluating the relative costs and benefits of rule-based and machine learning approaches in identifying whether specific procedures are covered, whether the coverage is applied after a deductible, and what the units of coverage are (e.g., per visit, per procedure).


## Background
The NCS collects detailed information about health insurance plans offered by employers, such as deductibles and copays. Typically, data collectors are given Summary Plan Descriptions (SPD), which are long, unstandardized pamphlets and they are asked to extract the relevant information. With the passage of the Patient Protection and Affordable Care Act, insurers are now required to provide Summary of Benefits and Coverage documents for all plans. These documents are much shorter and more standardized than SPDs. Given the standardization of these documents, the team wants to automate the extraction of relevant text information and the transformation of that text into tabular data.

If information can be extracted and codified into tabular data with a high degree of confidence, this would reduce the amount of time data collectors spend on transcribing these documents, allowing them to focus on other key aspects of their job such as seeking cooperation from respondents. This automated extraction can also reduce the number of extraction and keying errors.


The team wanted to be sure that the approach and results could be clearly explained to non-technical stakeholders, in order to build confidence in the system.
{: .ai-uca-feature }

<br />

## Responsible AI

So far, the team has worked to develop a rule-based system for classification of text extracted from SBCs. If that approach is not successful, they look to other, more technical approaches such as simple, transparent, machine learning approaches, including logistic regression and decision trees. The objective is to be able to explain to non-technical stakeholders and data collectors how the system arrives at predictions in order to build confidence in the system and help them spot situations that might lead the system to make poor predictions.


## Where we go from here
Future development will involve ensuring that the system is robust to edge cases, such as unusually formatted SBC documents and scanned PDFs that require OCR conversion. The breadth of text data that the program extracts may be expanded, allowing NCS to publish more detailed data about healthcare plans.
